{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "path_to_helper_files = os.path.join('..', 'py_files')\n",
    "base_saved_models_dir = os.path.join('..', 'saved_models' )\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(path_to_helper_files)\n",
    "\n",
    "\n",
    "import global_variables\n",
    "import dataset_helper\n",
    "import nnet_models\n",
    "import train_utilities\n",
    "\n",
    "device = global_variables.device;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bleu_scores(source_name, target_name, embed_dim, rnn_layers, source_rnn_type, attention):\n",
    "    \n",
    "    MAX_LEN = 48\n",
    "    batchSize = 128\n",
    "\n",
    "    \n",
    "    source_embed_dim = embed_dim\n",
    "    source_hidden_size = embed_dim\n",
    "\n",
    "    target_embed_dim= embed_dim\n",
    "    target_hidden_size = 2*embed_dim\n",
    "    source_rnn_layers = rnn_layers\n",
    "    target_rnn_layers = rnn_layers\n",
    "    \n",
    "    if source_name == 'vi' and target_name == 'en':\n",
    "            target_train_path = '../Data/iwslt-vi-en/train.tok.en'\n",
    "            source_train_path = '../Data/iwslt-vi-en/train.tok.vi'\n",
    "\n",
    "            target_val_path = '../Data/iwslt-vi-en/dev.tok.en'\n",
    "            source_val_path = '../Data/iwslt-vi-en/dev.tok.vi'\n",
    "\n",
    "            target_test_path = '../Data/iwslt-vi-en/test.tok.en'\n",
    "            source_test_path = '../Data/iwslt-vi-en/test.tok.vi'\n",
    "\n",
    "    elif source_name == 'zh' and target_name == 'en':\n",
    "            target_train_path = '../Data/iwslt-zh-en/train.tok.en'\n",
    "            source_train_path = '../Data/iwslt-zh-en/train.tok.zh'\n",
    "\n",
    "            target_val_path = '../Data/iwslt-zh-en/dev.tok.en'\n",
    "            source_val_path = '../Data/iwslt-zh-en/dev.tok.zh'\n",
    "\n",
    "            target_test_path = '../Data/iwslt-zh-en/test.tok.en'\n",
    "            source_test_path = '../Data/iwslt-zh-en/test.tok.zh'\n",
    "    else:\n",
    "            sys.exit(source_name+'->'+target_name+' is invalid!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    saved_models_dir = os.path.join(base_saved_models_dir, source_name+'2'+target_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pth_save_folder_name = source_name+'2'+target_name+'_' + \\\n",
    "                            'source_embed_dim='+str(source_embed_dim) +  \\\n",
    "                            '-source_hidden_size='+str(source_hidden_size) +  \\\n",
    "                            '-source_rnn_layers=' + str(source_rnn_layers) + \\\n",
    "                            '-source_rnn_type='+str(source_rnn_type)+ \\\n",
    "                            '-target_embed_dim='+str(target_embed_dim) + \\\n",
    "                            '-target_hidden_size='+str(target_hidden_size) + \\\n",
    "                            '-target_rnn_layers='+str(target_rnn_layers) + \\\n",
    "                            '-attention='+str(attention);\n",
    "    pth_saved_dir = os.path.join(saved_models_dir, pth_save_folder_name)\n",
    "\n",
    "\n",
    "\n",
    "    config_string = source_name+'2'+target_name+'\\n' + \\\n",
    "                            'source_embed_dim='+str(source_embed_dim) +  \\\n",
    "                            '\\n source_hidden_size='+str(source_hidden_size) +  \\\n",
    "                            '\\n source_rnn_layers=' + str(source_rnn_layers) + \\\n",
    "                            '\\n source_rnn_type='+str(source_rnn_type)+ \\\n",
    "                            '\\n target_embed_dim='+str(target_embed_dim) + \\\n",
    "                            '\\n target_hidden_size='+str(target_hidden_size) + \\\n",
    "                            '\\n target_rnn_layers='+str(target_rnn_layers) + \\\n",
    "                            '\\n attention='+str(attention);\n",
    "    \n",
    "    print(config_string)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    saved_language_model_dir = os.path.join(saved_models_dir, 'lang_obj')\n",
    "\n",
    "\n",
    "\n",
    "    dataset_dict = {'val': dataset_helper.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                                                                                            source_path = source_val_path, target_path = target_val_path, \n",
    "                                                                                            lang_obj_path = saved_language_model_dir, val = True), \n",
    "\n",
    "                    'test': dataset_helper.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                                                                                            source_path = source_test_path, target_path = target_test_path, \n",
    "                                                                                            lang_obj_path = saved_language_model_dir, val = True)} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dataloader_dict = {'val': DataLoader(dataset_dict['val'], batch_size = 1, \n",
    "                                                                            collate_fn = dataset_helper.vocab_collate_func_val,\n",
    "                                                                    shuffle = True, num_workers=0), \n",
    "                                        'test': DataLoader(dataset_dict['test'], batch_size = 1, \n",
    "                                                                            collate_fn = dataset_helper.vocab_collate_func_val,\n",
    "                                                                    shuffle = True, num_workers=0)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    encoder = nnet_models.EncoderRNN(dataset_dict['val'].source_lang_obj.n_words, \n",
    "                                                                     embed_dim = source_embed_dim, \n",
    "                                                                     hidden_size = source_hidden_size,\n",
    "                                                                     rnn_layers = source_rnn_layers, \n",
    "                                                                     rnn_type = source_rnn_type).to(device);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    decoder = nnet_models.DecoderRNN(dataset_dict['val'].target_lang_obj.n_words, \n",
    "                                                                                            embed_dim = target_embed_dim, \n",
    "                                                                                            hidden_size = target_hidden_size, \n",
    "                                                                                            n_layers = target_rnn_layers, \n",
    "                                                                                            attention = attention).to(device)   \n",
    "\n",
    "\n",
    "    encoder.load_state_dict(torch.load( os.path.join( pth_saved_dir, 'encoder.pth')))\n",
    "    decoder.load_state_dict(torch.load( os.path.join( pth_saved_dir, 'decoder.pth')))\n",
    "\n",
    "    val_bleu_greedy_with_unknown = train_utilities.validation_function(encoder, decoder, dataloader_dict['val'], dataset_dict['val'].target_lang_obj, keep_unk = True)\n",
    "    val_bleu_greedy_without_uknown = train_utilities.validation_function(encoder, decoder, dataloader_dict['val'], dataset_dict['val'].target_lang_obj, keep_unk = False)\n",
    "    val_bleu_beam_with_unknown = train_utilities.validation_beam_search(encoder, decoder, dataloader_dict['val'], dataset_dict['val'].target_lang_obj, beam_size = 3, keep_unk = True)\n",
    "    val_bleu_beam_without_unknown = train_utilities.validation_beam_search(encoder, decoder, dataloader_dict['val'], dataset_dict['val'].target_lang_obj, beam_size = 3, keep_unk = False)\n",
    "\n",
    "    test_bleu_greedy_with_unknown = train_utilities.validation_function(encoder, decoder, dataloader_dict['test'], dataset_dict['test'].target_lang_obj, keep_unk = True)\n",
    "    test_bleu_greedy_without_uknown = train_utilities.validation_function(encoder, decoder, dataloader_dict['test'], dataset_dict['test'].target_lang_obj, keep_unk = False)\n",
    "    test_bleu_beam_with_unknown = train_utilities.validation_beam_search(encoder, decoder, dataloader_dict['test'], dataset_dict['test'].target_lang_obj, beam_size = 3, keep_unk = True)\n",
    "    test_bleu_beam_without_unknown = train_utilities.validation_beam_search(encoder, decoder, dataloader_dict['test'], dataset_dict['test'].target_lang_obj, beam_size = 3, keep_unk = False)\n",
    "\n",
    "    result_string = 'val_bleu_greedy_with_unknown = ' + str(val_bleu_beam_with_unknown) + '\\n' + \\\n",
    "                    'val_bleu_greedy_without_uknown = ' + str(val_bleu_greedy_without_uknown) + '\\n' + \\\n",
    "                    'val_bleu_beam_with_unknown = ' + str(val_bleu_beam_with_unknown) + '\\n' + \\\n",
    "                    'val_bleu_beam_without_unknown = ' + str(val_bleu_beam_without_unknown) + '\\n' + \\\n",
    "                    'test_bleu_greedy_with_unknown = ' + str(test_bleu_beam_with_unknown) + '\\n' + \\\n",
    "                    'test_bleu_greedy_without_uknown = ' + str(test_bleu_greedy_without_uknown) + '\\n' + \\\n",
    "                    'test_bleu_beam_with_unknown = ' + str(test_bleu_beam_with_unknown) + '\\n' + \\\n",
    "                    'test_bleu_beam_without_unknown = ' + str(test_bleu_beam_without_unknown) + '\\n' \n",
    "    \n",
    "    print(result_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese to English Without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = 'zh'\n",
    "target_name = 'en'\n",
    "attention = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_rnn_type_array = ['lstm', 'gru']\n",
    "embed_dim_array = [256, 512]\n",
    "rnn_layers_array = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh2en\n",
      "source_embed_dim=256\n",
      " source_hidden_size=256\n",
      " source_rnn_layers=1\n",
      " source_rnn_type=lstm\n",
      " target_embed_dim=256\n",
      " target_hidden_size=512\n",
      " target_rnn_layers=1\n",
      " attention=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sm7582/.conda/envs/denoising/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "for source_rnn_type in source_rnn_type_array:\n",
    "    for embed_dim in embed_dim_array:\n",
    "        for rnn_layers in rnn_layers_array:\n",
    "\n",
    "            print_bleu_scores(source_name, target_name, embed_dim, rnn_layers, source_rnn_type, attention)\n",
    "            print('='*50)\n",
    "            print('\\n \\n \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese to English With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_rnn_type_array = ['lstm']\n",
    "embed_dim_array = [256, 512]\n",
    "rnn_layers_array = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source_rnn_type in source_rnn_type_array:\n",
    "    for embed_dim in embed_dim_array:\n",
    "        for rnn_layers in rnn_layers_array:\n",
    "\n",
    "            print_bleu_scores(source_name, target_name, embed_dim, rnn_layers, source_rnn_type, attention)\n",
    "            print('='*50)\n",
    "            print('\\n \\n \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:denoising]",
   "language": "python",
   "name": "conda-env-denoising-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
