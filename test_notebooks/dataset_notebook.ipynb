{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK_IDX = 2\n",
    "PAD_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = [\"SOS\",\"EOS\",\"UKN\",\"PAD\"]\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word.append(word)\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file):\n",
    "    f = open(file)\n",
    "    list_l = []\n",
    "    for line in f:\n",
    "        list_l.append(line.strip())\n",
    "    df = pd.DataFrame()\n",
    "    df['data'] = list_l\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(df, source_lang_obj, target_lang_obj):\n",
    "    for lan in ['source','target']:\n",
    "        indices_data = []\n",
    "        if lan=='source':\n",
    "            lang_obj = source_lang_obj\n",
    "        else:\n",
    "            lang_obj = target_lang_obj\n",
    "            \n",
    "        for tokens in df[lan+'_tokenized']:\n",
    "            \n",
    "            index_list = [lang_obj.word2index[token] if token in lang_obj.word2index else UNK_IDX for token in tokens]\n",
    "            index_list.append(EOS_token)\n",
    "            indices_data.append(index_list)\n",
    "            \n",
    "        df[lan+'_indized'] = indices_data\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_language_obj(source_name, source_lang_obj_path, source_data):\n",
    "    \n",
    "    if not os.path.exists(source_lang_obj_path):\n",
    "        os.makedirs(source_lang_obj_path)\n",
    "    \n",
    "    full_file_path = os.path.join(source_lang_obj_path, source_name+'_lang_obj.p')\n",
    "    \n",
    "    if os.path.isfile(full_file_path):\n",
    "        source_lang_obj = pickle.load( open( full_file_path, \"rb\" ) );\n",
    "    else:\n",
    "        source_lang_obj = Lang(source_name);\n",
    "        for line in source_data:\n",
    "            source_lang_obj.addSentence(line);\n",
    "        pickle.dump( source_lang_obj, open(full_file_path , \"wb\" ) )\n",
    "        \n",
    "    return source_lang_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_language_pairs(source_path, target_path, source_name = 'en', target_name = 'vi',\n",
    "                        lang_obj_path = '.', Max_Len = 10):\n",
    "    source = read_dataset(source_path);\n",
    "    target = read_dataset(target_path);\n",
    "    \n",
    "    main_df = pd.DataFrame();\n",
    "    main_df['source_data'] = source['data'];\n",
    "    main_df['target_data'] = target['data'];\n",
    "    \n",
    "    source_lang_obj = load_or_create_language_obj(source_name, lang_obj_path, main_df['source_data']);\n",
    "    target_lang_obj = load_or_create_language_obj(target_name, lang_obj_path, main_df['target_data']);\n",
    "    \n",
    "    for x in ['source', 'target']:\n",
    "        main_df[x+'_tokenized'] = main_df[x + \"_data\"].apply(lambda x:x.lower().split() );\n",
    "        main_df[x+'_len'] = main_df[x+'_tokenized'].apply(lambda x: len(x)+1) #+1 for EOS\n",
    "    \n",
    "    main_df = token2index_dataset(main_df, source_lang_obj, target_lang_obj);\n",
    "    \n",
    "    main_df = main_df[ np.logical_and( np.logical_and(main_df['source_len'] >=2, main_df['target_len'] >=2) , \n",
    "                                  np.logical_and( main_df['source_len'] <= Max_Len, main_df['target_len'] <= Max_Len) ) ];\n",
    "    \n",
    "    return main_df, source_lang_obj, target_lang_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train_path = 'Data/iwslt-vi-en/train.tok.en'\n",
    "vi_train_path = 'Data/iwslt-vi-en/train.tok.vi'\n",
    "\n",
    "en_val_path = 'Data/iwslt-vi-en/dev.tok.en'\n",
    "vi_val_path = 'Data/iwslt-vi-en/dev.tok.vi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, _, _ = load_language_pairs(en_val_path, vi_val_path, 'en', 'vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_data</th>\n",
       "      <th>target_data</th>\n",
       "      <th>source_tokenized</th>\n",
       "      <th>source_len</th>\n",
       "      <th>target_tokenized</th>\n",
       "      <th>target_len</th>\n",
       "      <th>source_indized</th>\n",
       "      <th>target_indized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And I was very proud .</td>\n",
       "      <td>Tôi đã rất tự_hào về đất_nước tôi .</td>\n",
       "      <td>[and, i, was, very, proud, .]</td>\n",
       "      <td>7</td>\n",
       "      <td>[tôi, đã, rất, tự_hào, về, đất_nước, tôi, .]</td>\n",
       "      <td>9</td>\n",
       "      <td>[16, 2, 6, 29, 30, 27, 1]</td>\n",
       "      <td>[5, 32, 33, 34, 35, 15, 5, 31, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I was so shocked .</td>\n",
       "      <td>Tôi đã bị sốc .</td>\n",
       "      <td>[i, was, so, shocked, .]</td>\n",
       "      <td>6</td>\n",
       "      <td>[tôi, đã, bị, sốc, .]</td>\n",
       "      <td>6</td>\n",
       "      <td>[2, 6, 121, 126, 27, 1]</td>\n",
       "      <td>[5, 32, 152, 153, 31, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>But many die .</td>\n",
       "      <td>Nhưng rất nhiều người đã chết .</td>\n",
       "      <td>[but, many, die, .]</td>\n",
       "      <td>5</td>\n",
       "      <td>[nhưng, rất, nhiều, người, đã, chết, .]</td>\n",
       "      <td>8</td>\n",
       "      <td>[42, 181, 125, 27, 1]</td>\n",
       "      <td>[51, 33, 40, 115, 32, 149, 31, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>She &amp;apos;s not North Korean . &amp;quot;</td>\n",
       "      <td>Nó không phải là người Bắc Triều_Tiên . \"</td>\n",
       "      <td>[she, &amp;apos;s, not, north, korean, ., &amp;quot;]</td>\n",
       "      <td>8</td>\n",
       "      <td>[nó, không, phải, là, người, bắc, triều_tiên, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>[954, 96, 80, 3465, 2, 27, 23, 1]</td>\n",
       "      <td>[185, 53, 29, 14, 115, 3504, 2, 31, 24, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>These girls were so lucky .</td>\n",
       "      <td>những cô gái này đã rất may_mắn .</td>\n",
       "      <td>[these, girls, were, so, lucky, .]</td>\n",
       "      <td>7</td>\n",
       "      <td>[những, cô, gái, này, đã, rất, may_mắn, .]</td>\n",
       "      <td>9</td>\n",
       "      <td>[483, 336, 130, 121, 337, 27, 1]</td>\n",
       "      <td>[128, 274, 349, 130, 32, 33, 350, 31, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              source_data  \\\n",
       "1                  And I was very proud .   \n",
       "9                      I was so shocked .   \n",
       "21                         But many die .   \n",
       "35  She &apos;s not North Korean . &quot;   \n",
       "38            These girls were so lucky .   \n",
       "\n",
       "                                  target_data  \\\n",
       "1         Tôi đã rất tự_hào về đất_nước tôi .   \n",
       "9                             Tôi đã bị sốc .   \n",
       "21            Nhưng rất nhiều người đã chết .   \n",
       "35  Nó không phải là người Bắc Triều_Tiên . \"   \n",
       "38          những cô gái này đã rất may_mắn .   \n",
       "\n",
       "                                 source_tokenized  source_len  \\\n",
       "1                   [and, i, was, very, proud, .]           7   \n",
       "9                        [i, was, so, shocked, .]           6   \n",
       "21                            [but, many, die, .]           5   \n",
       "35  [she, &apos;s, not, north, korean, ., &quot;]           8   \n",
       "38             [these, girls, were, so, lucky, .]           7   \n",
       "\n",
       "                                     target_tokenized  target_len  \\\n",
       "1        [tôi, đã, rất, tự_hào, về, đất_nước, tôi, .]           9   \n",
       "9                               [tôi, đã, bị, sốc, .]           6   \n",
       "21            [nhưng, rất, nhiều, người, đã, chết, .]           8   \n",
       "35  [nó, không, phải, là, người, bắc, triều_tiên, ...          10   \n",
       "38         [những, cô, gái, này, đã, rất, may_mắn, .]           9   \n",
       "\n",
       "                       source_indized  \\\n",
       "1           [16, 2, 6, 29, 30, 27, 1]   \n",
       "9             [2, 6, 121, 126, 27, 1]   \n",
       "21              [42, 181, 125, 27, 1]   \n",
       "35  [954, 96, 80, 3465, 2, 27, 23, 1]   \n",
       "38   [483, 336, 130, 121, 337, 27, 1]   \n",
       "\n",
       "                                target_indized  \n",
       "1            [5, 32, 33, 34, 35, 15, 5, 31, 1]  \n",
       "9                     [5, 32, 152, 153, 31, 1]  \n",
       "21           [51, 33, 40, 115, 32, 149, 31, 1]  \n",
       "35  [185, 53, 29, 14, 115, 3504, 2, 31, 24, 1]  \n",
       "38    [128, 274, 349, 130, 32, 33, 350, 31, 1]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguagePair(Dataset):\n",
    "    def __init__(self, source_name, target_name, source_path, target_path, \n",
    "                    lang_obj_path, max_len):\n",
    "        \n",
    "        self.source_name = source_name;\n",
    "        self.target_name = target_name; \n",
    "        \n",
    "        self.main_df, self.source_lang_obj, self.target_lang_obj = load_language_pairs(source_path, target_path, \n",
    "                                                                              source_name, target_name, lang_obj_path,\n",
    "                                                                              max_len);\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len( self.main_df )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return [self.main_df.iloc[idx]['source_indized'], self.main_df.iloc[idx]['target_indized'], \n",
    "                    self.main_df.iloc[idx]['source_len'], self.main_df.iloc[idx]['target_len'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset = LanguagePair('en', 'vi', en_val_path, vi_val_path, 'Language_Objects', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 208, 596, 8, 597, 16, 29, 598, 27, 1],\n",
       " [2, 595, 596, 8, 597, 20, 33, 598, 8, 1],\n",
       " 10,\n",
       " 10]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'got', 'dirty', ',', 'and', 'we', 'loved', 'it', '.', 'EOS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "source_data                          We got dirty , and we loved it .\n",
       "target_data                  Chúng_tôi lem_luốc , nhưng ai cũng vui .\n",
       "source_tokenized           [we, got, dirty, ,, and, we, loved, it, .]\n",
       "source_len                                                         10\n",
       "target_tokenized    [chúng_tôi, lem_luốc, ,, nhưng, ai, cũng, vui, .]\n",
       "target_len                                                          9\n",
       "source_indized            [33, 495, 3321, 8, 16, 33, 985, 253, 27, 1]\n",
       "target_indized                [38, 3458, 8, 51, 186, 230, 850, 31, 1]\n",
       "Name: 1081, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([temp_dataset.source_lang_obj.index2word[x] for x in temp_dataset[177][0] ])\n",
    "temp.iloc[177]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch, MAX_LEN):\n",
    "    source_data = []\n",
    "    target_data = []\n",
    "    source_len = []\n",
    "    target_len = []\n",
    "\n",
    "    for datum in batch:\n",
    "        source_len.append(datum[2])\n",
    "        target_len.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        if datum[2]>MAX_LEN:\n",
    "            padded_vec_s1 = np.array(datum[0])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s1 = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_LEN - datum[2])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        if datum[3]>MAX_LEN:\n",
    "            padded_vec_s2 = np.array(datum[1])[:MAX_LEN]\n",
    "        else:\n",
    "            padded_vec_s2 = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0,MAX_LEN - datum[3])),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        source_data.append(padded_vec_s1)\n",
    "        target_data.append(padded_vec_s2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(source_data)), torch.from_numpy(np.array(target_data)),\n",
    "            torch.from_numpy(np.array(source_len)), torch.from_numpy(np.array(target_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataloader = DataLoader(temp_dataset, batch_size=32, collate_fn = partial(vocab_collate_func, MAX_LEN=10),\n",
    "                                shuffle = True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
